{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install tensorflow-macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/FinalBalancedDataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant vectorisé les tweets, cela signifie donner une valeur numerique à chaques mot.\n",
    "Ces mots serons alors dans un dictionnaire dont on va décider de la capacité de celui (ici je vais prendre 150 000).\n",
    "On representera les tweets sous forme de tableau numpy. \n",
    "> Le dictionnaire que nous faisons est un vocabulaire compris uniquement par le model, on appel cela la tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 12:23:28.396056: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(56745, 1300), dtype=int64, numpy=\n",
       "array([[    2,    34,     4, ...,     0,     0,     0],\n",
       "       [    2,     2,   233, ...,     0,     0,     0],\n",
       "       [  115,    27,  4880, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  389,  5936,   225, ...,     0,     0,     0],\n",
       "       [ 9865,    56,  1185, ...,     0,     0,     0],\n",
       "       [18062, 44065, 63266, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecteur = TextVectorization(max_tokens=150_000,output_sequence_length=1300,output_mode='int')\n",
    "# le vecteur apprend mtn notre vocabulaire \n",
    "vecteur.adapt(df['tweet'].values)\n",
    "# Il faut mtn vectorisé nos tweets\n",
    "text_vectorise = vecteur(df['tweet'].values) \n",
    "text_vectorise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation du dataset\n",
    "Maintenant que le vocabulaire existe et que les tweets sont vectoriser, il nous reste plus qu'a creer le dataset afin de l'envoyer au model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cree le dataset avec les tweets vectorisés et les labels (on ajoute des options supplémentaire qui vont servire a l'entrainement)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_vectorise, df['Toxicity'].values)).cache().shuffle(60000).batch(10).prefetch(8)\n",
    "# Ici on va reperatir le contenu de notre dataset avec la partie entrainement 80%, evaluation 10% et test 10%\n",
    "octante = int(len(dataset)*.8)\n",
    "dix = int(len(dataset)*.1)\n",
    "train,val,test = (dataset.take(octante),dataset.skip(octante).take(dix),dataset.skip(octante+dix).take(dix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "le dataset est maintenant configuré, il est temps de creer notre model, de l'entrainer et de l'evaluer.\n",
    "Dans le cadre du NLP et dans notre cas la compréhension de phrase de leur sens, il va falloir configurer le model de manière spécifique: \n",
    "- le nombre de couche egal au nombre max mot dans notre dictionnaire (embedding)\n",
    "- une bidirectionnalité très importatn pour le sens des phrases\n",
    "- Des couches intermédiaies qui connectent l'ensemble des couches   \n",
    "- Enfin la couche final avec la fonction sigmoid qui va nous renvoyer une valeur entre 0 et 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(150_001, 32))\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4540/4540 [==============================] - 1123s 247ms/step - loss: 0.1619 - val_loss: 0.0944\n",
      "Epoch 2/5\n",
      "4540/4540 [==============================] - 1092s 241ms/step - loss: 0.0939 - val_loss: 0.0473\n",
      "Epoch 3/5\n",
      "4540/4540 [==============================] - 1110s 245ms/step - loss: 0.0569 - val_loss: 0.0267\n",
      "Epoch 4/5\n",
      "4540/4540 [==============================] - 1121s 247ms/step - loss: 0.0342 - val_loss: 0.0183\n",
      "Epoch 5/5\n",
      "4540/4540 [==============================] - 1136s 250ms/step - loss: 0.0216 - val_loss: 0.0147\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy())\n",
    "res_train = model.fit(train, epochs=5, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 557ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sauvegarde le model \n",
    "model.save('model_toxic2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# On sauvegarde le vecteur\n",
    "pickle.dump({'config': vecteur.get_config(),\n",
    "             'weights': vecteur.get_weights()}\n",
    "            , open(\"monVecteur.pkl\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
